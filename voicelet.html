<!DOCTYPE html>
<html>
<head>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
	<style>
		#listen, select {
			font-size: 24pt;
		}
		#text-area {
			font-size: 16pt;
		}
		#clear, #share  {
			font-size: 18pt;
		}

		@media screen and (max-width: 600px) {
			#clear, #share  {
				font-size: 12pt;
			}
			#text-area {
				font-size: 14pt;
			}
		}
		
		body {
			display: flex;
			flex-direction: column;
			height: 100vh;
			margin: 0;
		}
		#slider, #drop-down, #text-area, #listen, #clear, #share {
			margin: 5px;
			border: none;
			flex-grow: 0;
			flex-shrink: 0;

		}
		#slider { flex-basis: 5%;  }
		
		
		#drop-down  { flex-basis: 10%; color: #595959;}
		#text-area  { flex-basis: 50%; }
		
		#buttons-container { 
			display: flex;
			flex-direction: row; 
			justify-content: center; 
			align-items: center; 
			flex-wrap: wrap; 

			display: flex;
			flex-direction: row;
		}

		#clear, #share { 
			flex-basis: 15%; 
			min-height: 20pt;
			border-radius: 5px;
			background-color: #F2F2F2;
		}
		#listen { 
			flex-basis: 60%; 
			min-height: 40pt;
			border-radius: 100px;
			background-color: #7295DB;
			color: #fff;

		}
	</style>
</head>
<body>
	<input type="range" id="slider" />
	<select id="drop-down"></select>
	<textarea id="text-area"></textarea>
	<div id="buttons-container">
			<button id="clear">Clear</button>
			<button id="listen">Listen</button>
			<button id="share">Share</button>
	</div>
	
	<script>
		const slider = document.getElementById("slider");
		const dropDown = document.getElementById("drop-down");
		//dropDown.addEventListener('change', speakMessage);
		const textArea = document.getElementById("text-area");
		const recordButton = document.getElementById("listen");
		const clearConversation = document.getElementById("clear");
		var isRecording = false;
		var mediaRecorder;
		var recordedChunks = [];

		var messages = [];
		var tokens = 0;
		var modes = {
			'Concise factual': {pipeline: chat, params: {'temperature': 0}, messages: {
				system: "You are a concise assistant that answers in a way that is easy to listen by reading. No formalities or appologies."}},
			'Concise factual GPT4': {pipeline: chat, params: {'model': 'gpt-4', 'temperature': 0}, messages: {
				system: "You are a concise assistant that answers in a way that is easy to listen by reading. No formalities or appologies."}},
			'Pohádkobot': {pipeline: chat, params: {'temperature': 1}, messages: {
				system: "Jsi pohádkový robot. Vyprávíš pohádky dětem na jejich přání. Pokud není řečeno jinak, vymysli pohádku přibližně o sedmi odstavcích."}},
			'Repeat what I say': {pipeline: speak}
		};
		
		function log(text){
			textArea.value += text + " ";
		}
		
		const czechChars = /[ěščřžýáíéúůďťň]/i; 
		
		let voices = {}

		let key = new URLSearchParams(window.location.search).get('key');

		function getBestVoices(){
			window.speechSynthesis.getVoices().forEach(voice => {
				let lang = voice.lang.substring(0,2);
				if(! (lang in voices)){
					voices[lang] = voice;
				} else
					if(voice.name.toLowerCase().includes('premium')){
						voices[lang] = voice;
					}
			});
		}

		function setupUI() {
			for(role in modes){
				let option = document.createElement("option");
				option.textContent = role;
				option.value = role;
				dropDown.appendChild(option);
			}
			clear();
		}

		try { window.speechSynthesis.addEventListener("voiceschanged", getBestVoices) } catch {}

		if (typeof window.speechSynthesis === 'undefined') 
			log("Your phone has no voices:(");
		else
			getBestVoices();

		setupUI();
		

		function getVoiceByLang(str) {
			const substring = str.substring(0, 100);
			if (czechChars.test(substring))
				return voices['cs'];
			else
				return voices['en'];
		}

		function speak(text){
			if ('speechSynthesis' in window) {

				let rate = 0.7 + 0.6*(slider.value/100);
				const msg = new SpeechSynthesisUtterance();
				//msg.voice = speechSynthesis.getVoices().find(voice => voice.name === dropDown.value);
				msg.voice = getVoiceByLang(text);
				msg.text = text;
				msg.rate = rate;
				window.speechSynthesis.speak(msg);
			  } else {
				console.log('Speech synthesis not supported');
			  }
			  recordButton.textContent = "Listen";
		}


		recordButton.addEventListener("click", () => {
			if (!isRecording) {
				startRecording();
			} else {
				stopRecording();
			}
		});

		function clear(){
			let system = "You are a concise assistant that answers in a way that is easy to listen by reading. No formalities or appologies.";
			try {
				log(dropDown.value)
				messages = [{ role: 'system', content: modes[dropDown.value].messages.system}];
			} catch { messages = []; }

			textArea.value = "";
			clearConversation.textContent = "";
			tokens = 0;
		}

		clearConversation.addEventListener("click", clear);
		dropDown.addEventListener("change", clear);
	  
		async function startRecording() {
			isRecording = true;
			recordButton.textContent = "STOP listening and Answer!";

			const mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
			mediaRecorder = new MediaRecorder(mediaStream);
			recordedChunks = [];
			mediaRecorder.start();

			mediaRecorder.addEventListener('dataavailable', event => {
				if (event.data.size > 0) {
					recordedChunks.push(event.data);
				}
			});
		}

		async function stopRecording() {
			isRecording = false;
			recordButton.textContent = "…recognizing";
			mediaRecorder.stop();

			try {  
				mediaRecorder.addEventListener("stop", async () => {
					let type = "audio/mpeg"; //audio/mpeg, audio/mp4, audio/x-m4a, not audio/webm
					let extension = ".mp3"
					const audioBlob = new Blob(recordedChunks, {type: type}); 
					const audioUrl = URL.createObjectURL(audioBlob);
					const audioFile = new File([audioBlob], "recordedAudio"+extension, {type: type});

					query = await transcribe(audioFile);
					
					if(query.hasOwnProperty('text')){	
						textArea.value = query.text;
						modes[dropDown.value].pipeline(query.text)
						//pipeline(query.text);
					}
					if(query.hasOwnProperty('error')){
						log("Error: "+query['error']['message']);
						speak(query['error']['message']);
					}
				});
			} catch (e) {
				log(e);
			}
		}

		async function transcribe(file){
			formData = new FormData();
			formData.append('file', file);
			formData.append('model', 'whisper-1')
			try {
				const transcription = await fetch("https://api.openai.com/v1/audio/transcriptions", {
					method: 'POST',
					headers: {
						'Authorization': "Bearer "+key //,"Content-Type": "multipart/form-data"
					},
					body: formData //,model: "whisper-1"
				});
				const query = await transcription.json();
				return (query);
			} catch(e){
				log(e);
				return {'text': "Error in voice recognition"}
			}
		}


		async function chat(question){
			messages.push({ role: 'user', content: query.text });
			recordButton.textContent = "…thinking";

			let params = modes[dropDown.value].params;
			if(!('model' in params))
				params.model = 'gpt-3.5-turbo';
			params.messages = messages;

			const response = await fetch("https://api.openai.com/v1/chat/completions", {
				method: 'POST',
				headers: {
					'Content-Type': 'application/json',
					'Authorization': 'Bearer '+key
				},
				body: JSON.stringify(params)
			});

			const reply = await response.json();

			try {
				message = reply['choices'][0]['message']
				messages.push(message);
				tokens += reply['usage']['total_tokens'];
				clearConversation.textContent = "Clear " + tokens + " words";

				log(" >>> " + message['content']);
			} catch (e) {
				log(e);
			}
			
			//textArea.value += message['content'];
			speak(message['content']);
			//textArea.value +=  usage['total_tokens']

		}

	</script>
</body>
</html>